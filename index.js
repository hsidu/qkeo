import express from "express";
import OpenAI from "openai";
import dotenv from "dotenv";

dotenv.config(); // โ ุชุญููู ูุชุบูุฑุงุช ุงูุจูุฆุฉ

const app = express();
app.use(express.json()); // โ ุฏุนู JSON ูู ุงูุทูุจุงุช

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY, // โ ููุชุงุญ OpenAI
});

let chatHistory = [{ role: "system", content: "You are a helpful assistant." }];

// โ ุงุณุชูุจุงู ุงูุฑุณุงุฆู ูุญูุธูุง
app.post("/chat", (req, res) => {
  const content = req.body.message;
  chatHistory.push({ role: "user", content: content });
  res.status(200).json({ success: true });
});

// โ ุฅุนุงุฏุฉ ุถุจุท ุงูุณุฌู
app.post("/reset", (req, res) => {
  chatHistory = [{ role: "system", content: "You are a helpful assistant." }];
  res.status(200).json({ success: true });
});

// โ ุนุฑุถ ุณุฌู ุงููุญุงุฏุซุฉ
app.get("/history", (req, res) => {
  res.status(200).json(chatHistory);
});

// โ ุจุซ ูุจุงุดุฑ ููุฑุฏูุฏ
app.get("/stream", async (req, res) => {
  res.setHeader("Content-Type", "text/event-stream");
  res.setHeader("Cache-Control", "no-cache");
  res.setHeader("Connection", "keep-alive");

  try {
    const stream = await openai.beta.chat.completions.stream({
      model: "gpt-3.5-turbo",
      messages: chatHistory,
      stream: true,
    });

    for await (const chunk of stream) {
      const message = chunk.choices[0]?.delta?.content || "";
      res.write(`data: ${JSON.stringify(message)}\n\n`);
    }

    res.end();
  } catch (error) {
    res.write(`event: error\ndata: ${JSON.stringify({ message: "Stream encountered an error" })}\n\n`);
    res.end();
  }

  req.on("close", () => {
    res.end();
  });
});

// โ ุชุดุบูู ุงูุณูุฑูุฑ ุนูู ุงููููุฐ ุงูุตุญูุญ
const PORT = process.env.PORT || 3000;
app.listen(PORT, () => {
  console.log(`๐ ุงูุณูุฑูุฑ ูุนูู ุนูู ุงููููุฐ ${PORT}`);
});
